{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WMT22-MasaKhane SPC",
      "provenance": [],
      "collapsed_sections": [
        "HZbxqzuO_HFL",
        "-rQP8gw5W8oT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Mount Drive"
      ],
      "metadata": {
        "id": "HZbxqzuO_HFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBCOPMiL9p7E",
        "outputId": "c414575d-3a09-4b26-f848-4284a9433545"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clone WMT22-MasaKhane GitHub Repo"
      ],
      "metadata": {
        "id": "-rQP8gw5W8oT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h1GUsEtWZqS",
        "outputId": "a3faf921-9812-4a0f-9989-506deaf9ee57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n",
            "Cloning into 'WMT22-MasaKhane'...\n",
            "remote: Enumerating objects: 120, done.\u001b[K\n",
            "remote: Counting objects: 100% (120/120), done.\u001b[K\n",
            "remote: Compressing objects: 100% (93/93), done.\u001b[K\n",
            "remote: Total 120 (delta 58), reused 66 (delta 20), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (120/120), 2.15 MiB | 9.43 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive\n",
        "\n",
        "!git clone https://github.com/abumafrim/WMT22-MasaKhane.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set-up\n",
        "\n",
        "*   Change directory to WMT22-MasaKhane\n",
        "*   Install required libraries"
      ],
      "metadata": {
        "id": "9YctIEqpXW3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/WMT22-MasaKhane\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lnJvAXKXXbpZ",
        "outputId": "19213eb8-1d4b-418b-f9c8-5583d3a8cb45"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/WMT22-MasaKhane\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==3.1.0\n",
            "  Downloading transformers-3.1.0-py3-none-any.whl (884 kB)\n",
            "\u001b[K     |████████████████████████████████| 884 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting datasets==1.0.1\n",
            "  Downloading datasets-1.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 71.2 MB/s \n",
            "\u001b[?25hCollecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from -r sentence-pair-classification/requirements.txt (line 4)) (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (from -r sentence-pair-classification/requirements.txt (line 5)) (0.5.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from -r sentence-pair-classification/requirements.txt (line 6)) (1.11.0+cu113)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r sentence-pair-classification/requirements.txt (line 7)) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r sentence-pair-classification/requirements.txt (line 8)) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r sentence-pair-classification/requirements.txt (line 9)) (1.3.5)\n",
            "Collecting argparse\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r sentence-pair-classification/requirements.txt (line 14)) (1.4.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from -r sentence-pair-classification/requirements.txt (line 15)) (0.0)\n",
            "Collecting sh\n",
            "  Downloading sh-1.14.2-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 7.1 MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc2\n",
            "  Downloading tokenizers-0.8.1rc2-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0 MB 66.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r sentence-pair-classification/requirements.txt (line 1)) (2022.6.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r sentence-pair-classification/requirements.txt (line 1)) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r sentence-pair-classification/requirements.txt (line 1)) (3.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r sentence-pair-classification/requirements.txt (line 1)) (2.23.0)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.1.0->-r sentence-pair-classification/requirements.txt (line 1)) (4.64.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 80.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1->-r sentence-pair-classification/requirements.txt (line 2)) (0.3.5.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets==1.0.1->-r sentence-pair-classification/requirements.txt (line 2)) (6.0.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 78.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0->-r sentence-pair-classification/requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0->-r sentence-pair-classification/requirements.txt (line 1)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0->-r sentence-pair-classification/requirements.txt (line 1)) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.1.0->-r sentence-pair-classification/requirements.txt (line 1)) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->-r sentence-pair-classification/requirements.txt (line 6)) (4.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r sentence-pair-classification/requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r sentence-pair-classification/requirements.txt (line 7)) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r sentence-pair-classification/requirements.txt (line 7)) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r sentence-pair-classification/requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->-r sentence-pair-classification/requirements.txt (line 7)) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r sentence-pair-classification/requirements.txt (line 9)) (2022.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->-r sentence-pair-classification/requirements.txt (line 15)) (1.0.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r sentence-pair-classification/requirements.txt (line 1)) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.1.0->-r sentence-pair-classification/requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r sentence-pair-classification/requirements.txt (line 15)) (3.1.0)\n",
            "Building wheels for collected packages: gputil, sacremoses\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=670dbdd38aacb6b51b3fd44aba8b4b52d27f741ec9e1342512a1fffa3d6b520a\n",
            "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=7d55c25a8bdb43dd64053e5306fb4225c368ab8493177c2485473ae946aad90f\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built gputil sacremoses\n",
            "Installing collected packages: xxhash, tokenizers, sentencepiece, sacremoses, transformers, sh, gputil, datasets, argparse\n",
            "Successfully installed argparse-1.4.0 datasets-1.0.1 gputil-1.4.0 sacremoses-0.0.53 sentencepiece-0.1.96 sh-1.14.2 tokenizers-0.8.1rc2 transformers-3.1.0 xxhash-3.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download data\n",
        "\n",
        "* MAFAND-MT\n",
        "* Huggingface LASER\n",
        "* Create Sentence-pair classification dataset"
      ],
      "metadata": {
        "id": "F64mtQHXXy1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/WMT22-MasaKhane\n",
        "\n",
        "#Clone mafand and preprocess data files\n",
        "!git clone https://github.com/masakhane-io/lafand-mt.git\n",
        "!python3 download-and-process-mafand.py\n",
        "\n",
        "#Download Hugginface LASER and preprocess data files\n",
        "!python3 download-and-process-hug-laser.py\n",
        "\n",
        "%cd sentence-pair-classification\n",
        "!python3 create-spc-data.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sl5wqKtYAMk",
        "outputId": "a8fe842e-f9cb-438f-baac-ab52aff511b2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/WMT22-MasaKhane\n",
            "/content/drive/MyDrive/WMT22-MasaKhane/sentence-pair-classification\n",
            "Creation of the eng-lug folder...\n",
            "Finished: eng-lug\n",
            "Creation of the eng-tsn folder...\n",
            "Finished: eng-tsn\n",
            "Creation of the eng-zul folder...\n",
            "Finished: eng-zul\n",
            "Creation of the fra-wol folder...\n",
            "Finished: fra-wol\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train SPC Model"
      ],
      "metadata": {
        "id": "utV8sdOVaX7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/WMT22-MasaKhane/sentence-pair-classification\n",
        "\n",
        "import os\n",
        "\n",
        "##Provide the model to finetune\n",
        "## Any of \"albert-base-v2\", \"albert-large-v2\", \"albert-xlarge-v2\", \"albert-xxlarge-v2\", \"bert-base-uncased\", etc\n",
        "\n",
        "model = 'albert-base-v2'\n",
        "\n",
        "#hug_langs = ['eng-hau', 'eng-ibo', 'eng-lug', 'eng-swh', 'eng-tsn', 'eng-yor', 'eng-zul', 'fra-wol']\n",
        "#maf_langs = ['en_hau', 'en_ibo', 'en_lug', 'en_swa', 'en_tsn', 'en_yor', 'en_zul', 'fr_wol']\n",
        "\n",
        "hug_langs = ['eng-lug', 'eng-tsn', 'eng-zul', 'fra-wol']\n",
        "maf_langs = ['en_lug', 'en_tsn', 'en_zul', 'fr_wol']\n",
        "\n",
        "for hug_lang, maf_lang in zip(hug_langs, maf_langs):\n",
        "  train_path = 'data/' + hug_lang + '/spc-' + maf_lang + '_train.tsv'\n",
        "  dev_path = 'data/' + hug_lang + '/spc-' + maf_lang + '_dev.tsv'\n",
        "  test_path = 'data/' + hug_lang + '/spc-' + maf_lang + '_test.tsv'\n",
        "\n",
        "  data_to_classify = 'data/' + hug_lang + '/spc-' + maf_lang + '_to_classify.tsv'\n",
        "\n",
        "  model_path = 'models/' + hug_lang\n",
        "  if not os.path.exists(model_path):\n",
        "    print(\"Creation of the \" + hug_lang + \" model folder...\")\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "  !python3 run-sp-class.py \\\n",
        "      --train \\\n",
        "      --eval=True \\\n",
        "      --model={model} \\\n",
        "      --model_path={model_path} \\\n",
        "      --train_data={train_path} \\\n",
        "      --val_data={dev_path} \\\n",
        "      --test_data={test_path} \\\n",
        "      --epochs=4\n",
        " \n",
        "  val_loss = 100\n",
        "\n",
        "  for x in os.listdir(model_path):\n",
        "    if x.endswith(\".pt\"):\n",
        "      if float(x[x.index('loss') + 5:x.index('loss') + 9]) < val_loss:\n",
        "        val_loss = float(x[x.index('loss') + 5:x.index('loss') + 9])\n",
        "        model_name = model_path + '/' + x\n",
        "\n",
        "  output_path = 'data/' + hug_lang\n",
        "\n",
        "  !python3 predict.py \\\n",
        "      --predict \\\n",
        "      --model={model} \\\n",
        "      --model_path={model_name} \\\n",
        "      --data_path={data_to_classify} \\\n",
        "      --output_path={output_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljz93MnAabD1",
        "outputId": "18303ade-1b94-4750-cad7-48b217abf438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/WMT22-MasaKhane/sentence-pair-classification\n",
            "PyTorch version 1.11.0+cu113 available.\n",
            "TensorFlow version 2.8.2 available.\n",
            "Gen RAM Free: 87.4 GB  | Proc size: 798.8 MB\n",
            "GPU RAM Free: 40536MB | Used: 0MB | Util   0% | Total 40536MB\n",
            "Reading training data...\n",
            "Downloading: 100% 684/684 [00:00<00:00, 612kB/s]\n",
            "Downloading: 100% 760k/760k [00:00<00:00, 7.46MB/s]\n",
            "Reading validation data...\n",
            "Downloading: 100% 47.4M/47.4M [00:00<00:00, 68.3MB/s]\n",
            " 20% 101/510 [00:06<00:23, 17.69it/s]\n",
            "Iteration 102/510 of epoch 1 complete. Loss : 0.17076843892972843 \n",
            " 40% 203/510 [00:11<00:16, 18.64it/s]\n",
            "Iteration 204/510 of epoch 1 complete. Loss : 0.09903182465510041 \n",
            " 60% 305/510 [00:17<00:11, 18.46it/s]\n",
            "Iteration 306/510 of epoch 1 complete. Loss : 0.0787077126796266 \n",
            " 80% 407/510 [00:22<00:05, 17.97it/s]\n",
            "Iteration 408/510 of epoch 1 complete. Loss : 0.07042094746915002 \n",
            "100% 509/510 [00:28<00:00, 18.48it/s]\n",
            "Iteration 510/510 of epoch 1 complete. Loss : 0.07026049139790748 \n",
            "100% 510/510 [00:28<00:00, 17.80it/s]\n",
            "100% 188/188 [00:04<00:00, 41.06it/s]\n",
            "\n",
            "Epoch 1 complete! Validation Loss : 0.11599423968173722\n",
            "Best validation loss improved from inf to 0.11599423968173722\n",
            "\n",
            " 20% 101/510 [00:05<00:22, 18.47it/s]\n",
            "Iteration 102/510 of epoch 2 complete. Loss : 0.05963789230194308 \n",
            " 40% 203/510 [00:11<00:16, 18.24it/s]\n",
            "Iteration 204/510 of epoch 2 complete. Loss : 0.04838664598954732 \n",
            " 60% 305/510 [00:16<00:10, 18.66it/s]\n",
            "Iteration 306/510 of epoch 2 complete. Loss : 0.0449743799890355 \n",
            " 80% 407/510 [00:22<00:05, 18.46it/s]\n",
            "Iteration 408/510 of epoch 2 complete. Loss : 0.04414951321486311 \n",
            "100% 509/510 [00:27<00:00, 19.10it/s]\n",
            "Iteration 510/510 of epoch 2 complete. Loss : 0.035417983408871236 \n",
            "100% 510/510 [00:27<00:00, 18.44it/s]\n",
            "100% 188/188 [00:04<00:00, 42.33it/s]\n",
            "\n",
            "Epoch 2 complete! Validation Loss : 0.09038687552860443\n",
            "Best validation loss improved from 0.11599423968173722 to 0.09038687552860443\n",
            "\n",
            " 20% 100/510 [00:05<00:21, 19.06it/s]\n",
            "Iteration 102/510 of epoch 3 complete. Loss : 0.0350238885901704 \n",
            " 40% 202/510 [00:10<00:16, 18.79it/s]\n",
            "Iteration 204/510 of epoch 3 complete. Loss : 0.027955910620078735 \n",
            " 60% 304/510 [00:16<00:11, 17.92it/s]\n",
            "Iteration 306/510 of epoch 3 complete. Loss : 0.022932521427077624 \n",
            " 80% 406/510 [00:22<00:05, 17.73it/s]\n",
            "Iteration 408/510 of epoch 3 complete. Loss : 0.025523256060813425 \n",
            "100% 508/510 [00:27<00:00, 17.54it/s]\n",
            "Iteration 510/510 of epoch 3 complete. Loss : 0.01947903567377259 \n",
            "100% 510/510 [00:28<00:00, 18.17it/s]\n",
            "100% 188/188 [00:04<00:00, 40.61it/s]\n",
            "\n",
            "Epoch 3 complete! Validation Loss : 0.09688377494309494\n",
            " 20% 101/510 [00:05<00:22, 18.13it/s]\n",
            "Iteration 102/510 of epoch 4 complete. Loss : 0.02187138493971753 \n",
            " 40% 203/510 [00:11<00:17, 17.71it/s]\n",
            "Iteration 204/510 of epoch 4 complete. Loss : 0.010231008640189162 \n",
            " 60% 305/510 [00:17<00:11, 17.81it/s]\n",
            "Iteration 306/510 of epoch 4 complete. Loss : 0.014263759595433287 \n",
            " 80% 407/510 [00:23<00:05, 17.47it/s]\n",
            "Iteration 408/510 of epoch 4 complete. Loss : 0.012052938733569473 \n",
            "100% 509/510 [00:29<00:00, 17.83it/s]\n",
            "Iteration 510/510 of epoch 4 complete. Loss : 0.006491429378465731 \n",
            "100% 510/510 [00:29<00:00, 17.43it/s]\n",
            "100% 188/188 [00:04<00:00, 42.05it/s]\n",
            "\n",
            "Epoch 4 complete! Validation Loss : 0.10385760877459765\n",
            "The model has been saved in models/eng-lug/albert-base-v2_lr_2e-05_val_loss_0.09039_ep_2.pt\n",
            "Reading test data...\n",
            "\n",
            "Loading the weights of the model...\n",
            "Predicting on test data...\n",
            "100% 188/188 [00:04<00:00, 41.27it/s]\n",
            "\n",
            "Predictions are available in : models/eng-lug/test_predictions.txt\n",
            "https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/tmpr83xvspo\n",
            "Downloading: 4.39kB [00:00, 3.29MB/s]       \n",
            "storing https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py in cache at /root/.cache/huggingface/datasets/50d5843bbbbd80c47809bc76a5b03c0fd87d068509b0060103ae8182e4f5cfb9.ec871b06a00118091ec63eff0a641fddcb8d3c7cd52e855bbb2be28944df4b82.py\n",
            "creating metadata file for /root/.cache/huggingface/datasets/50d5843bbbbd80c47809bc76a5b03c0fd87d068509b0060103ae8182e4f5cfb9.ec871b06a00118091ec63eff0a641fddcb8d3c7cd52e855bbb2be28944df4b82.py\n",
            "Checking /root/.cache/huggingface/datasets/50d5843bbbbd80c47809bc76a5b03c0fd87d068509b0060103ae8182e4f5cfb9.ec871b06a00118091ec63eff0a641fddcb8d3c7cd52e855bbb2be28944df4b82.py for additional imports.\n",
            "Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6\n",
            "Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6/glue.py\n",
            "Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/dataset_infos.json\n",
            "Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6/glue.json\n",
            "\n",
            "{'accuracy': 0.9477015323117921, 'f1': 0.9467616141064769}\n",
            "Gen RAM Free: 87.4 GB  | Proc size: 772.7 MB\n",
            "GPU RAM Free: 40536MB | Used: 0MB | Util   0% | Total 40536MB\n",
            "Reading data...\n",
            "\n",
            "Loading the weights of the model...\n",
            "Predicting quality of parallel data...\n",
            " 73% 39478/53916 [27:41<10:08, 23.72it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "%cd /content/drive/MyDrive/WMT22 MasaKhane/MasaKhaneNLP-WMT22/sentence-pair-classification\n",
        "\n",
        "#hug_langs = ['eng-hau', 'eng-ibo', 'eng-lug', 'eng-swh', 'eng-tsn', 'eng-yor', 'eng-zul', 'fra-wol']\n",
        "#maf_langs = ['en_hau', 'en_ibo', 'en_lug', 'en_swa', 'en_tsn', 'en_yor', 'en_zul', 'fr_wol']\n",
        "\n",
        "hug_langs = ['eng-lug', 'eng-tsn', 'eng-zul', 'fra-wol']\n",
        "maf_langs = ['en_lug', 'en_tsn', 'en_zul', 'fr_wol']\n",
        "\n",
        "thresholds = [0.4, 0.5, 0.7]\n",
        "\n",
        "for hug_lang, maf_lang in zip(hug_langs, maf_langs):\n",
        "\n",
        "  with open('data/' + hug_lang + '/predictions.txt', 'r') as f:\n",
        "      preds = f.readlines()\n",
        "\n",
        "  df_pred = pd.read_csv('data/' + hug_lang + '/spc-' + maf_lang + '_to_classify.tsv', sep='\\t')\n",
        "\n",
        "  for threshold in thresholds:\n",
        "\n",
        "    src_correct = []\n",
        "    tgt_correct = []\n",
        "\n",
        "    for sent1, sent2, pred in zip(df_pred['sentence1'], df_pred['sentence2'], preds):\n",
        "      if float(pred) >= threshold:\n",
        "        src_correct.append(sent1)\n",
        "        tgt_correct.append(sent2)\n",
        "\n",
        "    df = pd.DataFrame(list(zip(src_correct, tgt_correct)), columns=['input', 'target'])\n",
        "    df.to_csv(os.path.join('data/' + hug_lang + '/', 'correct-translations_t_' + str(threshold) + '.tsv'), sep='\\t', index=False)\n",
        "\n",
        "    print(lang, threshold, len(df))"
      ],
      "metadata": {
        "id": "bazd6Rozd0kE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}