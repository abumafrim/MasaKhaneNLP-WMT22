{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WMT22-MasaKhane SPC",
      "provenance": [],
      "collapsed_sections": [
        "HZbxqzuO_HFL",
        "-rQP8gw5W8oT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Mount Drive"
      ],
      "metadata": {
        "id": "HZbxqzuO_HFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sBCOPMiL9p7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Clone WMT22-MasaKhane GitHub Repo"
      ],
      "metadata": {
        "id": "-rQP8gw5W8oT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h1GUsEtWZqS"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive\n",
        "\n",
        "!git clone https://github.com/abumafrim/WMT22-MasaKhane.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set-up\n",
        "\n",
        "*   Change directory to WMT22-MasaKhane\n",
        "*   Install required libraries"
      ],
      "metadata": {
        "id": "9YctIEqpXW3f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/WMT22-MasaKhane\n",
        "\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "lnJvAXKXXbpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download data\n",
        "\n",
        "* MAFAND-MT\n",
        "* Huggingface LASER\n",
        "* Create Sentence-pair classification dataset"
      ],
      "metadata": {
        "id": "F64mtQHXXy1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/WMT22-MasaKhane\n",
        "\n",
        "#Clone mafand and preprocess data files\n",
        "!git clone https://github.com/masakhane-io/lafand-mt.git\n",
        "!python3 download-and-process-mafand.py\n",
        "\n",
        "#Download Hugginface LASER and preprocess data files\n",
        "!python3 download-and-process-hug-laser.py\n",
        "\n",
        "%cd sentence-pair-classification\n",
        "!python3 create-spc-data.py"
      ],
      "metadata": {
        "id": "2sl5wqKtYAMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train SPC Model"
      ],
      "metadata": {
        "id": "utV8sdOVaX7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/WMT22-MasaKhane/sentence-pair-classification\n",
        "\n",
        "import os\n",
        "\n",
        "##Provide the model to finetune\n",
        "## Any of \"albert-base-v2\", \"albert-large-v2\", \"albert-xlarge-v2\", \"albert-xxlarge-v2\", \"bert-base-uncased\", etc\n",
        "\n",
        "model = 'albert-base-v2'\n",
        "\n",
        "#hug_langs = ['eng-hau', 'eng-ibo', 'eng-lug', 'eng-swh', 'eng-tsn', 'eng-yor', 'eng-zul', 'fra-wol']\n",
        "#maf_langs = ['en_hau', 'en_ibo', 'en_lug', 'en_swa', 'en_tsn', 'en_yor', 'en_zul', 'fr_wol']\n",
        "\n",
        "hug_langs = ['eng-lug', 'eng-tsn', 'eng-zul', 'fra-wol']\n",
        "maf_langs = ['en_lug', 'en_tsn', 'en_zul', 'fr_wol']\n",
        "\n",
        "for hug_lang, maf_lang in zip(hug_langs, maf_langs):\n",
        "  train_path = 'data/' + hug_lang + '/spc-' + maf_lang + '_train.tsv'\n",
        "  dev_path = 'data/' + hug_lang + '/spc-' + maf_lang + '_dev.tsv'\n",
        "  test_path = 'data/' + hug_lang + '/spc-' + maf_lang + '_test.tsv'\n",
        "\n",
        "  data_to_classify = 'data/' + hug_lang + '/spc-' + maf_lang + '_to_classify.tsv'\n",
        "\n",
        "  model_path = 'models/' + hug_lang\n",
        "  if not os.path.exists(model_path):\n",
        "    print(\"Creation of the \" + hug_lang + \" model folder...\")\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "  !python3 run-sp-class.py \\\n",
        "      --train \\\n",
        "      --eval=True \\\n",
        "      --model={model} \\\n",
        "      --model_path={model_path} \\\n",
        "      --train_data={train_path} \\\n",
        "      --val_data={dev_path} \\\n",
        "      --test_data={test_path} \\\n",
        "      --epochs=4\n",
        " \n",
        "  val_loss = 100\n",
        "\n",
        "  for x in os.listdir(model_path):\n",
        "    if x.endswith(\".pt\"):\n",
        "      if float(x[x.index('loss') + 5:x.index('loss') + 9]) < val_loss:\n",
        "        val_loss = float(x[x.index('loss') + 5:x.index('loss') + 9])\n",
        "        model_name = model_path + '/' + x\n",
        "\n",
        "  output_path = 'data/' + hug_lang\n",
        "\n",
        "  !python3 predict.py \\\n",
        "      --predict \\\n",
        "      --model={model} \\\n",
        "      --model_path={model_name} \\\n",
        "      --data_path={data_to_classify} \\\n",
        "      --output_path={output_path}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljz93MnAabD1",
        "outputId": "3703f55a-391d-4cfe-941d-ab1f4de54157"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/WMT22-MasaKhane/sentence-pair-classification\n",
            "PyTorch version 1.11.0+cu113 available.\n",
            "TensorFlow version 2.8.2 available.\n",
            "Gen RAM Free: 87.4 GB  | Proc size: 798.8 MB\n",
            "GPU RAM Free: 40536MB | Used: 0MB | Util   0% | Total 40536MB\n",
            "Reading training data...\n",
            "Downloading: 100% 684/684 [00:00<00:00, 612kB/s]\n",
            "Downloading: 100% 760k/760k [00:00<00:00, 7.46MB/s]\n",
            "Reading validation data...\n",
            "Downloading: 100% 47.4M/47.4M [00:00<00:00, 68.3MB/s]\n",
            " 20% 101/510 [00:06<00:23, 17.69it/s]\n",
            "Iteration 102/510 of epoch 1 complete. Loss : 0.17076843892972843 \n",
            " 40% 203/510 [00:11<00:16, 18.64it/s]\n",
            "Iteration 204/510 of epoch 1 complete. Loss : 0.09903182465510041 \n",
            " 60% 305/510 [00:17<00:11, 18.46it/s]\n",
            "Iteration 306/510 of epoch 1 complete. Loss : 0.0787077126796266 \n",
            " 80% 407/510 [00:22<00:05, 17.97it/s]\n",
            "Iteration 408/510 of epoch 1 complete. Loss : 0.07042094746915002 \n",
            "100% 509/510 [00:28<00:00, 18.48it/s]\n",
            "Iteration 510/510 of epoch 1 complete. Loss : 0.07026049139790748 \n",
            "100% 510/510 [00:28<00:00, 17.80it/s]\n",
            "100% 188/188 [00:04<00:00, 41.06it/s]\n",
            "\n",
            "Epoch 1 complete! Validation Loss : 0.11599423968173722\n",
            "Best validation loss improved from inf to 0.11599423968173722\n",
            "\n",
            " 20% 101/510 [00:05<00:22, 18.47it/s]\n",
            "Iteration 102/510 of epoch 2 complete. Loss : 0.05963789230194308 \n",
            " 40% 203/510 [00:11<00:16, 18.24it/s]\n",
            "Iteration 204/510 of epoch 2 complete. Loss : 0.04838664598954732 \n",
            " 60% 305/510 [00:16<00:10, 18.66it/s]\n",
            "Iteration 306/510 of epoch 2 complete. Loss : 0.0449743799890355 \n",
            " 80% 407/510 [00:22<00:05, 18.46it/s]\n",
            "Iteration 408/510 of epoch 2 complete. Loss : 0.04414951321486311 \n",
            "100% 509/510 [00:27<00:00, 19.10it/s]\n",
            "Iteration 510/510 of epoch 2 complete. Loss : 0.035417983408871236 \n",
            "100% 510/510 [00:27<00:00, 18.44it/s]\n",
            "100% 188/188 [00:04<00:00, 42.33it/s]\n",
            "\n",
            "Epoch 2 complete! Validation Loss : 0.09038687552860443\n",
            "Best validation loss improved from 0.11599423968173722 to 0.09038687552860443\n",
            "\n",
            " 20% 100/510 [00:05<00:21, 19.06it/s]\n",
            "Iteration 102/510 of epoch 3 complete. Loss : 0.0350238885901704 \n",
            " 40% 202/510 [00:10<00:16, 18.79it/s]\n",
            "Iteration 204/510 of epoch 3 complete. Loss : 0.027955910620078735 \n",
            " 60% 304/510 [00:16<00:11, 17.92it/s]\n",
            "Iteration 306/510 of epoch 3 complete. Loss : 0.022932521427077624 \n",
            " 80% 406/510 [00:22<00:05, 17.73it/s]\n",
            "Iteration 408/510 of epoch 3 complete. Loss : 0.025523256060813425 \n",
            "100% 508/510 [00:27<00:00, 17.54it/s]\n",
            "Iteration 510/510 of epoch 3 complete. Loss : 0.01947903567377259 \n",
            "100% 510/510 [00:28<00:00, 18.17it/s]\n",
            "100% 188/188 [00:04<00:00, 40.61it/s]\n",
            "\n",
            "Epoch 3 complete! Validation Loss : 0.09688377494309494\n",
            " 20% 101/510 [00:05<00:22, 18.13it/s]\n",
            "Iteration 102/510 of epoch 4 complete. Loss : 0.02187138493971753 \n",
            " 40% 203/510 [00:11<00:17, 17.71it/s]\n",
            "Iteration 204/510 of epoch 4 complete. Loss : 0.010231008640189162 \n",
            " 60% 305/510 [00:17<00:11, 17.81it/s]\n",
            "Iteration 306/510 of epoch 4 complete. Loss : 0.014263759595433287 \n",
            " 80% 407/510 [00:23<00:05, 17.47it/s]\n",
            "Iteration 408/510 of epoch 4 complete. Loss : 0.012052938733569473 \n",
            "100% 509/510 [00:29<00:00, 17.83it/s]\n",
            "Iteration 510/510 of epoch 4 complete. Loss : 0.006491429378465731 \n",
            "100% 510/510 [00:29<00:00, 17.43it/s]\n",
            "100% 188/188 [00:04<00:00, 42.05it/s]\n",
            "\n",
            "Epoch 4 complete! Validation Loss : 0.10385760877459765\n",
            "The model has been saved in models/eng-lug/albert-base-v2_lr_2e-05_val_loss_0.09039_ep_2.pt\n",
            "Reading test data...\n",
            "\n",
            "Loading the weights of the model...\n",
            "Predicting on test data...\n",
            "100% 188/188 [00:04<00:00, 41.27it/s]\n",
            "\n",
            "Predictions are available in : models/eng-lug/test_predictions.txt\n",
            "https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/tmpr83xvspo\n",
            "Downloading: 4.39kB [00:00, 3.29MB/s]       \n",
            "storing https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py in cache at /root/.cache/huggingface/datasets/50d5843bbbbd80c47809bc76a5b03c0fd87d068509b0060103ae8182e4f5cfb9.ec871b06a00118091ec63eff0a641fddcb8d3c7cd52e855bbb2be28944df4b82.py\n",
            "creating metadata file for /root/.cache/huggingface/datasets/50d5843bbbbd80c47809bc76a5b03c0fd87d068509b0060103ae8182e4f5cfb9.ec871b06a00118091ec63eff0a641fddcb8d3c7cd52e855bbb2be28944df4b82.py\n",
            "Checking /root/.cache/huggingface/datasets/50d5843bbbbd80c47809bc76a5b03c0fd87d068509b0060103ae8182e4f5cfb9.ec871b06a00118091ec63eff0a641fddcb8d3c7cd52e855bbb2be28944df4b82.py for additional imports.\n",
            "Creating main folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue\n",
            "Creating specific version folder for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6\n",
            "Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py to /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6/glue.py\n",
            "Couldn't find dataset infos file at https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/dataset_infos.json\n",
            "Creating metadata file for metric https://raw.githubusercontent.com/huggingface/datasets/1.0.1/metrics/glue/glue.py at /root/.cache/huggingface/modules/datasets_modules/metrics/glue/19382a5758e4e23ecb68fc5a724e719f425492cd21d2aba0db5053ec14cac0d6/glue.json\n",
            "\n",
            "{'accuracy': 0.9477015323117921, 'f1': 0.9467616141064769}\n",
            "Gen RAM Free: 87.4 GB  | Proc size: 772.7 MB\n",
            "GPU RAM Free: 40536MB | Used: 0MB | Util   0% | Total 40536MB\n",
            "Reading data...\n",
            "\n",
            "Loading the weights of the model...\n",
            "Predicting quality of parallel data...\n",
            " 87% 46783/53916 [32:48<05:00, 23.70it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "%cd /content/drive/MyDrive/WMT22 MasaKhane/MasaKhaneNLP-WMT22/sentence-pair-classification\n",
        "\n",
        "#hug_langs = ['eng-hau', 'eng-ibo', 'eng-lug', 'eng-swh', 'eng-tsn', 'eng-yor', 'eng-zul', 'fra-wol']\n",
        "#maf_langs = ['en_hau', 'en_ibo', 'en_lug', 'en_swa', 'en_tsn', 'en_yor', 'en_zul', 'fr_wol']\n",
        "\n",
        "hug_langs = ['eng-lug', 'eng-tsn', 'eng-zul', 'fra-wol']\n",
        "maf_langs = ['en_lug', 'en_tsn', 'en_zul', 'fr_wol']\n",
        "\n",
        "thresholds = [0.4, 0.5, 0.7]\n",
        "\n",
        "for hug_lang, maf_lang in zip(hug_langs, maf_langs):\n",
        "\n",
        "  with open('data/' + hug_lang + '/predictions.txt', 'r') as f:\n",
        "      preds = f.readlines()\n",
        "\n",
        "  df_pred = pd.read_csv('data/' + hug_lang + '/spc-' + maf_lang + '_to_classify.tsv', sep='\\t')\n",
        "\n",
        "  for threshold in thresholds:\n",
        "\n",
        "    src_correct = []\n",
        "    tgt_correct = []\n",
        "\n",
        "    for sent1, sent2, pred in zip(df_pred['sentence1'], df_pred['sentence2'], preds):\n",
        "      if float(pred) >= threshold:\n",
        "        src_correct.append(sent1)\n",
        "        tgt_correct.append(sent2)\n",
        "\n",
        "    df = pd.DataFrame(list(zip(src_correct, tgt_correct)), columns=['input', 'target'])\n",
        "    df.to_csv(os.path.join('data/' + hug_lang + '/', 'correct-translations_t_' + str(threshold) + '.tsv'), sep='\\t', index=False)\n",
        "\n",
        "    print(lang, threshold, len(df))"
      ],
      "metadata": {
        "id": "bazd6Rozd0kE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}